# -*- coding: utf-8 -*-
"""Github_RFM_online_retail.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xtHIkfa7Nj3CsPtT9ddUObQpVmV2UgHC
"""



"""Remenber to set the spark version correctly. Here I used spark-2.4.7
Find a version available on this site: https://downloads.apache.org/spark/

###Set up spark environment
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz
!tar xf spark-2.4.7-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.7-bin-hadoop2.7"

import findspark
findspark.init()
from pyspark.sql import SparkSession
# spark = SparkSession.builder.master("local[*]").getOrCreate()

from pyspark.sql import SparkSession

from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType, TimestampType, IntegerType
from pyspark.sql.functions import to_date

from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.feature import VectorAssembler

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""###Import and Explore Dataset"""

spark = SparkSession \
    .builder \
    .appName('Ruoyuan\'s Python Spark Notebook') \
    .getOrCreate()
print(spark.version)  
sc = spark.sparkContext

!wget https://raw.githubusercontent.com/ruoyuanDu/Online_sales-RFM/main/online_retail_II.csv

raw_data = spark.read.csv('online_retail_II.csv', header = True)

raw_data_df = pd.read_csv('online_retail_II.csv')
raw_data_df.head()

raw_data.count()

raw_data.printSchema()

raw_data_df.info()

"""### Check if any outliers and unusual values are in the dataset"""

figure, axes = plt.subplots(1,2,sharex=False, sharey=False, figsize=(8, 5),squeeze=True)
axes[0].plot(raw_data_df['Price'], '.', label = 'Price')
axes[1].plot(raw_data_df['Quantity'], '.', label='Quantity')
plt.legend()
plt.show()

"""Price and quantity values cannot be negative, so we need to remove negative points

### Data Clean up

pyspark dataframe: change column data types
"""

raw_data = raw_data.withColumn('Quantity', raw_data.Quantity.cast(IntegerType()))\
                   .withColumn('Price', raw_data.Price.cast(DoubleType()))\
                   .withColumn('InvoiceDate', to_date('InvoiceDate', 'MM/dd/yyyy H:m'))

raw_data.printSchema()

raw_data.show()

"""Drop rows with null values"""

clean_raw_data = raw_data.filter(raw_data['Invoice'].isNotNull())\
                         .filter(raw_data['InvoiceDate'].isNotNull())\
                         .filter(raw_data['Price'].isNotNull())\
                         .filter(raw_data['Quantity'].isNotNull())\
                         .filter(raw_data['Customer ID'].isNotNull())

clean_raw_data.count()

"""Drop negative price and quantity values"""

clean_raw_data.filter(F.col('Quantity')<0).count()

clean_raw_data.filter(F.col('Price')<0).count()

clean_raw_data = clean_raw_data.filter(F.col('Quantity')>=0)

clean_raw_data.count()

"""Group by customer ID and calculate the sum of total values spent by each customer, find their most recent purchase, and total number of purchases have been made"""

clean_raw_data_agg = clean_raw_data.groupBy('Customer ID').agg(F.round(F.sum(F.col('Quantity')*F.col('Price')),3).alias('Total expense'),
                                                               F.countDistinct(F.col('Invoice')).alias('Total number of purchases'),
                                                               F.max('InvoiceDate').alias('Most recent purchase'))

clean_raw_data_agg.show()

clean_raw_data_agg.agg(F.max('Total expense')).show()

clean_raw_data_agg.sort('Total expense', ascending=False).show()

"""From the table above, some of the total expense is much larger than the rest values, we need further check those extrem values

Convert pyspark dataframe to pandas dataframe so that we can visually check outliers
"""

clean_raw_data_agg_df = clean_raw_data_agg.toPandas()

"""### Check Outliers"""

plt.boxplot(clean_raw_data_agg_df['Total expense'])
plt.show()

plt.boxplot(clean_raw_data_agg_df['Total number of purchases'])
plt.show()



"""### Using Inter quantile range method to determine outliers"""

# Define a function to calculate upper and lower bounds using inter quantile range method
def iqr_bound(dataframe, columns):
  '''columns must be in the form of a list'''
  bounds = dict()
  for c in columns:
    bounds[c] = dict(zip(['q1', 'q3'], dataframe.approxQuantile(c, [0.25, 0.75], 0)))
    iqr = bounds[c]['q3']-bounds[c]['q1']
    bounds[c]['lower'] = bounds[c]['q1']-iqr*1.5
    bounds[c]['upper'] = bounds[c]['q3']+iqr*1.5
  return bounds

bounds = iqr_bound(clean_raw_data_agg, ['Total expense','Total number of purchases'])

bounds

"""Even though there are many values are byond upper bounds of Total expense and total number of purchases, these values cannot be deleted as outliers easily because no evidense showing their irrationality.

To examine how these values can affect our model, we'd like to create two dataframes, with and without values beyond upper bounds.
"""

clean_data_agg_without_outliers = clean_raw_data_agg.withColumn('outlier_total_expense',F.when((F.col('Total expense')<bounds['Total expense']['lower'])|(F.col('Total expense')>bounds['Total expense']['upper']), 1)\
                                                   .otherwise(0))

clean_data_agg_without_outliers.show()

clean_data_agg_without_outliers = clean_data_agg_without_outliers.filter(F.col('outlier_total_expense')==0)

clean_data_agg_without_outliers.show()

df  = clean_data_agg_without_outliers.toPandas()

plt.hist(df['Total expense'], bins=20)
plt.xlim([0, 6000])
plt.plot()

"""#### Calculation of frequency
Date difference
"""

clean_raw_data_agg = clean_raw_data_agg.withColumn('date_difference', F.datediff(to_date(F.lit('2011-12-31')),F.col('Most recent purchase')))

clean_raw_data_agg.show()

"""**Assign groups**

Set up cutoffs by percentile on the amount of money spent:[0.85, 0.7, 0.4, 0]

"""

monetary_cutoffs = clean_raw_data_agg.approxQuantile('Total expense',[0.75, 0.5, 0.25],0)

monetary_cutoffs

recency_cutoffs = clean_raw_data_agg.approxQuantile('date_difference', [0.75, 0.5, 0.25], 0)
recency_cutoffs

frequency_cutoffs = clean_raw_data_agg.approxQuantile('Total number of purchases', [0.75, 0.5,0.25],0)
frequency_cutoffs

clean_raw_data_agg_levels = clean_raw_data_agg.withColumn('Total_expense_level', 
                              F.when(F.col('Total expense')>monetary_cutoffs[0],1)\
                              .when(F.col('Total expense')>monetary_cutoffs[1],2)\
                              .when(F.col('Total expense')>monetary_cutoffs[2],3)\
                              .otherwise(4))\
                  .withColumn('Recency_level', F.when(F.col('date_difference')<recency_cutoffs[2],1)\
                                                .when(F.col('date_difference')<recency_cutoffs[1],2)\
                                                .when(F.col('date_difference')<recency_cutoffs[0],3)\
                                                .otherwise(4))\
                  .withColumn('Frequency_level',F.when(F.col('Total number of purchases')>frequency_cutoffs[0],1)\
                                                 .when(F.col('Total number of purchases')>frequency_cutoffs[1],2)\
                                                 .when(F.col('Total number of purchases')>frequency_cutoffs[2],3)\
                                                 .otherwise(4))

clean_raw_data_agg_levels.filter(F.col('Frequency_level')==4).show()

"""**Check the number of customers in group R-1, F-1, M-1** 

customers in group 111 are considered to be best customers becuase they made purchase recently, do so often and psend more than other customers.
"""

best_customers = clean_raw_data_agg_levels\
                         .filter(F.col('Total_expense_level')==1)\
                         .filter(F.col('Recency_level')==1)\
                         .filter(F.col('Frequency_level')==1).count()

"""**Check the number of customers in group R-1,F-4, M-1 and group R-1, F-4, M-2** 

Customers in these 2 groups are considered to be high-spending new customersã€‚ These are customers who made purchases only once, but very recently and they spend a lot
"""

high_spending_new_customers = clean_raw_data_agg_levels.filter((F.col('Total_expense_level')==2)|(F.col('Total_expense_level')==1))\
                                                       .filter(F.col('Recency_level')==1)\
                                                       .filter((F.col('Frequency_level')==4))

high_spending_new_customers.count()

"""**Check the number of customers in group R-1,F-1, M-3 and group R-1, F-1, M-4**   

Customers in these 2 groups are considered to be lowest-spending active loyal customers becuase they made purchases frequencytly and do so often, but spend the least


"""

lowest_spending_loyal_customers = clean_raw_data_agg_levels.filter(((F.col('Total_expense_level')==3)|(F.col('Total_expense_level')==4))&(F.col('Recency_level')==1)&(F.col('Frequency_level')==1))

lowest_spending_loyal_customers.count()

"""**Check the number of cusotmers in group R4-1-1, 4-1-2, 4-2-1 and 4-2-2** 

This segment consists of those customers that made purchases frequently and spent a lot, but it's been a long time since their last purchase
"""

churned_best_customers = clean_raw_data_agg_levels.filter((F.col('Recency_level')==4)&(F.col('Frequency_level')==1)|(F.col('Frequency_level')==2)&(F.col('Total_expense_level')==1)|(F.col('Total_expense_level')==2))

churned_best_customers.count()

"""### Scaling features before fitting intto Kmeans models"""

from pyspark.ml import Pipeline

columns = ['Total expense','Total number of purchases', 'date_difference']
assemblers=[VectorAssembler(inputCols=[col], outputCol=col+"_vec") for col in columns]
scalers = [MinMaxScaler(inputCol = col+"_vec", outputCol=col+"_scaled") for col in columns]
pipeline = Pipeline(stages=assemblers+scalers)
scalerModel=pipeline.fit(clean_raw_data_agg_levels)
scaledData=scalerModel.transform(clean_raw_data_agg_levels)

scaledData.printSchema()

#filtering out unecessary columsn
scaledata = scaledData.select('Customer ID','Total expense','Total number of purchases','date_difference','Total expense_scaled','Total number of purchases_scaled','date_difference_scaled')

scaledata.show()

"""### KNN"""

vecAssembler = VectorAssembler(inputCols=['Total expense_scaled','Total number of purchases_scaled','date_difference_scaled'], outputCol='features')
new_df = vecAssembler.transform(scaledata)

from pyspark.ml.clustering import KMeans
kmeans = KMeans().setK(64).setSeed(1)
kmeans_clf = kmeans.fit(new_df.select('features'))
transformed= kmeans_clf.transform(new_df)

transformed.groupBy('prediction').count().show()

centers = kmeans_clf.clusterCenters()

centers

